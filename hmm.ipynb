{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Vocabulary Creation\n",
    "In this task, we will create a vocabulary using the training data. We will replace all words with occurences less than a threshold (default: 3) to a special token \\<unk>. Finally, we will export this vocabulary to a file named vocab.txt where each line is in the format of 'word\\tindex\\toccurences', where index starts at 0 and occurences ordered in descending order. Moreover, the first line should be the \\<unk> token with index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the word frequencies in the given file without using any threshold\n",
    "def count_word_frequencies(fname):\n",
    "    word_frequencies = {}\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # Read one word of the sentence at a time\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                arguments = line.split('\\t')\n",
    "                if len(arguments) != 3:\n",
    "                    print(f'Invalid format on line: {line}')\n",
    "                else:\n",
    "                    # arguments: [index, word, pos_tag]\n",
    "                    word = arguments[1]\n",
    "                    if word in word_frequencies:\n",
    "                        word_frequencies[word] += 1\n",
    "                    else:\n",
    "                        word_frequencies[word] = 1\n",
    "    return word_frequencies\n",
    "\n",
    "# create the vocab using word frequencies of all the words with a cutoff threshold\n",
    "# any words with frequency lower than the threshold will be omitted and counted as the speical token <unk>\n",
    "def create_vocab(word_frequencies, threshold=3):\n",
    "    vocab = {}\n",
    "    # Add special token <unk> which includes all the removed token (those with occurence less than the threshold)\n",
    "    unk_frequency = 0\n",
    "    for word, frequency in word_frequencies.items():\n",
    "        if frequency >= threshold:\n",
    "            vocab[word] = frequency\n",
    "        else:\n",
    "            unk_frequency += frequency\n",
    "    return vocab, unk_frequency\n",
    "\n",
    "# exports the vocabulary and frequency of <unk> tokens into a file where the each line is in the format of\n",
    "#'word\\tindex\\toccurences', where index starts at 0 and occurences ordered in descending order\n",
    "# also, the first line should be the <unk> token with index 0\n",
    "def export_vocab(fname, vocab, unk_frequency):\n",
    "    with open(fname, 'w') as f:\n",
    "        # First line is <unk> special token\n",
    "        f.write(f'<unk>\\t0\\t{unk_frequency}\\n')\n",
    "        # sort the vocab in descending order of frequency\n",
    "        sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "        # write each word in the format 'word\\tindex\\toccurences'\n",
    "        for index, (word, frequency) in enumerate(sorted_vocab):\n",
    "            f.write(f'{word}\\t{index+1}\\t{frequency}')\n",
    "            # Add new line if not on the last entry\n",
    "            if index < len(sorted_vocab)-1:\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter for word cutoff based on occurences\n",
    "threshold = 10\n",
    "\n",
    "word_frequencies = count_word_frequencies('data/train')\n",
    "vocab, unk_frequency = create_vocab(word_frequencies, threshold=threshold)\n",
    "export_vocab('vocab.txt', vocab, unk_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of vocabulary: 7096\n",
      "Occurences of <unk>: 80313\n"
     ]
    }
   ],
   "source": [
    "print(f'Total size of vocabulary: {len(vocab)}')\n",
    "print(f'Occurences of <unk>: {unk_frequency}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "I tried experimenting with the threshold hyperparameter and found that the lower the threshold, the higher accuracy I obtain: (0.94 greedy acc with threshold=2 vs 0.932 greedy acc with threshold=4). However, I do not believe a lower threshold would be better if met with a testing set that does not share many of the words. As such, having some recollection of how to deal with \\<unk> would probably be better in the long run.\n",
    "\n",
    "Moreover, I tested text pre-processing such as converting all words to lowercase, but found that this harmed the accuracy aswell. My justification for this is that we are sometimes introducing ambiguity by doing this, for example the NN 'Will' vs MD 'will' will have one record but are entirely different. Will will write his will. This becomes even more confusing and the results reflect that, and that's why I refrained from lowercasing the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model Learning\n",
    "In this task we will create an HMM from the training data. The closed-form for the emission and transition parameters in HMM are $$t(s*|s) = count(s->s*)/count(s)$$ $$e(x|s) = count(s->x)/count(s)$$ respectively, where $s$ is current tag, $s*$ is next tag, and x is the current word. After calculating these probabilities, we export them as two dictionaries to 'hmm.json'. The first dictionary, transition, contains items with pairs of $(s,s*)$ as key and $t(s*|s)$ as value. The second dictionary, emission, contains items with pairs of $(s,x)$ as key and $e(x|s)$ as value.\n",
    "\n",
    "Note that when we parse the sentences from the training data, we inject two special tokens, \\<start> and \\<end>, at the start and end of each sentence respectively. This trick aids us in computing the probabilities for the hmm, specifically t(s1) becomes t(s1|\\<start>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Parses the given file using the specified vocab\n",
    "# All words not in the vocab will be replaced with <unk> word\n",
    "# Returns list of sentences where each sentence has two dictionaries, 'words' and 'tags'\n",
    "# This can only be used on training/dev files as it expects the line to have the format: index word tag\n",
    "def read_sentences(fname, vocab):\n",
    "    sentences = []\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        words = ['<start>']\n",
    "        tags = ['<start>']\n",
    "        # each line represents a single token (word, punct, number, etc) in a sentence\n",
    "        for line in lines:\n",
    "            # Read one word of the sentence at a time\n",
    "            line = line.strip()\n",
    "            # On non-empty lines, parse line, which is index\\tword\\ttag\n",
    "            if line:\n",
    "                # arguments = [index, word, tag]\n",
    "                arguments = line.split('\\t')\n",
    "                # Skip on bad lines\n",
    "                if len(arguments) != 3:\n",
    "                    print(f'Invalid format on line: {line}')\n",
    "                    continue\n",
    "                else:\n",
    "                    word = arguments[1]\n",
    "                    tag = arguments[2]\n",
    "                    # only add the word if its in the vocab\n",
    "                    if word in vocab:\n",
    "                        words.append(word)\n",
    "                        tags.append(tag)\n",
    "                    # otherwise, add <unk> token\n",
    "                    else:\n",
    "                        words.append('<unk>')\n",
    "                        tags.append(tag) # TODO: is it tag or <unk>?\n",
    "            # On empty lines, it indicates a new sentence\n",
    "            else:\n",
    "                # Add end tag to the end of the sentence\n",
    "                words.append('<end>')\n",
    "                tags.append('<end>')\n",
    "                \n",
    "                # add words/tags to the sentences\n",
    "                sentences.append({'words': words, 'tags': tags})\n",
    "                \n",
    "                # reset sentence for the next sentence\n",
    "                words = ['<start>']\n",
    "                tags = ['<start>']\n",
    "    return sentences\n",
    "\n",
    "# Given a list of sentences, this will calculate the occurences of all tags\n",
    "def calc_tag_frequencies(sentences):\n",
    "    tag_frequencies = {}\n",
    "    for sentence in sentences:\n",
    "        tags = sentence['tags']\n",
    "        for tag in tags:\n",
    "            if tag in tag_frequencies:\n",
    "                tag_frequencies[tag] += 1\n",
    "            else:\n",
    "                tag_frequencies[tag] = 1\n",
    "    return tag_frequencies\n",
    "\n",
    "# Given a list of sentences and tag frequencies, this will create two dictionaries, transition and emission\n",
    "def create_hmm(sentences, tag_frequencies):\n",
    "    transition = {}\n",
    "    emission = {}\n",
    "    # Count the occurences for each transition/emission (excluding denominator normalization)\n",
    "    for sentence in sentences:\n",
    "        words = sentence['words']\n",
    "        tags = sentence['tags']\n",
    "        for i in range(len(words)):\n",
    "            # Skip the first/last word/tag as it is an injected <start>/<end> token\n",
    "            if i == 0 or i == len(words) - 1:\n",
    "                continue\n",
    "            word = words[i]\n",
    "            tag = tags[i]\n",
    "            prev_tag = tags[i-1]\n",
    "            # transition\n",
    "            transition_key = (prev_tag, tag)\n",
    "            if transition_key in transition:\n",
    "                transition[transition_key] += 1\n",
    "            else:\n",
    "                transition[transition_key] = 1\n",
    "            # emission\n",
    "            emission_key = (tag, word)\n",
    "            if emission_key in emission:\n",
    "                emission[emission_key] += 1\n",
    "            else:\n",
    "                emission[emission_key] = 1\n",
    "\n",
    "    # Normalize the transition into probabilities using the denominator\n",
    "    for key, value in transition.items():\n",
    "        # key: (s, s')\n",
    "        # value: t(s'|s)\n",
    "        prev_tag, tag = key\n",
    "        transition[key] = value / tag_frequencies[prev_tag]\n",
    "        \n",
    "    # Normalize the emission into probabilities using the denominator\n",
    "    for key, value in emission.items():\n",
    "        # key: (s, x)\n",
    "        # value: e(x|s)\n",
    "        tag, word = key\n",
    "        emission[key] = value / tag_frequencies[tag]\n",
    "    \n",
    "    return transition, emission\n",
    "\n",
    "# Exports the transition and emission probabilities to the specified filename as json format\n",
    "def export_hmm(fname, transition, emission):\n",
    "    with open(fname, 'w') as f:\n",
    "        # Convert the tuple key to string key to be able to export as json\n",
    "        t = {str(k): v for k, v in transition.items()}\n",
    "        e = {str(k): v for k, v in emission.items()}\n",
    "        json.dump({'transition': t, 'emission': e}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse in the training data and filter using our earlier vocab\n",
    "sentences = read_sentences('data/train', vocab)\n",
    "\n",
    "# Calculate the tag frequencies, i.e. count(s) for all s\n",
    "tag_frequencies = calc_tag_frequencies(sentences)\n",
    "\n",
    "# Compute the transition and emission probabilities and then export to the file\n",
    "transition, emission = create_hmm(sentences, tag_frequencies)\n",
    "export_hmm('hmm.json', transition, emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Parameters in HMM: 1392\n",
      "Emission Parameters in HMM: 11118\n"
     ]
    }
   ],
   "source": [
    "transition_params = len(transition)\n",
    "emission_params = len(emission)\n",
    "\n",
    "print(f'Transition Parameters in HMM: {transition_params}')\n",
    "print(f'Emission Parameters in HMM: {emission_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding with HMM\n",
    "Here we implement and evalute the greedy decoding algorithm on the development data and report the accuracy. Then, we predict the pos tags on the test data and export the result into a file with format similar to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses the given file using the specified vocab\n",
    "# All words not in the vocab will be replaced with <unk> word\n",
    "# Returns list of sentences where each sentence has one dictionary, 'words'\n",
    "# This can only be used on test file as it expects the line to have the format: index word\n",
    "def read_sentences_test(fname, vocab):\n",
    "    sentences = []\n",
    "    with open(fname, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        words = ['<start>']\n",
    "        # each line represents a single token (word, punct, number, etc) in a sentence\n",
    "        for line in lines:\n",
    "            # Read one word of the sentence at a time\n",
    "            line = line.strip()\n",
    "            # On non-empty lines, parse line, which is index\\tword\n",
    "            if line:\n",
    "                # arguments = [index, word]\n",
    "                arguments = line.split('\\t')\n",
    "                # Skip on bad lines\n",
    "                if len(arguments) != 2:\n",
    "                    print(f'Invalid format on line: {line}')\n",
    "                    continue\n",
    "                else:\n",
    "                    word = arguments[1]\n",
    "                    # only add the word if its in the vocab\n",
    "                    if word in vocab:\n",
    "                        words.append(word)\n",
    "                    # otherwise, add <unk> token\n",
    "                    else:\n",
    "                        words.append('<unk>')\n",
    "            # On empty lines, it indicates a new sentence\n",
    "            else:\n",
    "                # Add end tag to the end of the sentence\n",
    "                words.append('<end>')\n",
    "                \n",
    "                # add words/tags to the sentences\n",
    "                sentences.append({'words': words})\n",
    "                \n",
    "                # reset sentence for the next sentence\n",
    "                words = ['<start>']\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Accuracy:  0.9078\n"
     ]
    }
   ],
   "source": [
    "# Parse in the development data and filter using our earlier vocab\n",
    "sentences = read_sentences('data/dev', vocab)\n",
    "\n",
    "# Keep counters to calculate accuracy\n",
    "total_correct = 0\n",
    "total = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence['words']\n",
    "    tags = sentence['tags'] # used only for accuracy measurement\n",
    "    \n",
    "    # for each word,\n",
    "    # find s* that maximizes t(s | s_prev) * e(word | s) where s_prev is the last predicted tag\n",
    "    # To simplify our model, we skip the first word (<start>) but predict the tag <start> at the beginning\n",
    "    # this is so the first 'real' tag, i.e. NN Dog, can follow the formula easily instead of having two cases\n",
    "    # where one case is t(s) and the other is t(s | s_prev)\n",
    "    predicted_tags = ['<start>']\n",
    "    for i in range(len(words)):\n",
    "        # skip the first/last word which is an injected token <start>/<end>\n",
    "        if i == 0 or i == len(words)-1:\n",
    "            continue\n",
    "        \n",
    "        word = words[i]\n",
    "        \n",
    "        # Keep counter of which tag has the best likelihood\n",
    "        best_tag = None\n",
    "        best_tag_value = 0\n",
    "        \n",
    "        # Enumerate through all possible transitions, and only consider those that fit the formula\n",
    "        # and those that do, we calculate its likelihood, and if its better than our best, we set it to our current best\n",
    "        for key, value in transition.items():\n",
    "            # key: (s, s')\n",
    "            # value: t(s'|s)\n",
    "            prev_tag, tag = key\n",
    "            \n",
    "            # skip the transitions that are not starting from our last predicted tag\n",
    "            if prev_tag != predicted_tags[-1]:\n",
    "                continue\n",
    "                \n",
    "            t = value # t(s' | s*) where s* is last predicted tag\n",
    "            e = 0\n",
    "            if (tag, word) in emission:\n",
    "                e = emission[(tag, word)]\n",
    "            computed = t * e\n",
    "            \n",
    "            if best_tag is None or computed > best_tag_value:\n",
    "                best_tag = tag\n",
    "                best_tag_value = computed\n",
    "        \n",
    "        # After looping through all transitions, we have found the best tag to predict\n",
    "        predicted_tags.append(best_tag)\n",
    "    predicted_tags.append('<end>')\n",
    "    \n",
    "    # Remove start/end tokens from the tags for a fair unpadded accuracy measurement\n",
    "    tags = tags[1:-1]\n",
    "    predicted_tags = predicted_tags[1:-1]\n",
    "    \n",
    "    # Compare actual tags with our predicted tags, and calculate the accuracy\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i] == predicted_tags[i]:\n",
    "            total_correct += 1\n",
    "    total += len(tags)\n",
    "    \n",
    "print(f'Greedy Accuracy: {total_correct / total : .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse in the test data and filter using our earlier vocab\n",
    "sentences = read_sentences_test('data/test', vocab)\n",
    "\n",
    "# A list to contain all of our sentence pos predictions\n",
    "output = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence['words']\n",
    "    \n",
    "    # for each word,\n",
    "    # find s* that maximizes t(s | s_prev) * e(word | s) where s_prev is the last predicted tag\n",
    "    # To simplify our model, we skip the first word (<start>) but predict the tag <start> at the beginning\n",
    "    # this is so the first 'real' tag, i.e. NN Dog, can follow the formula easily instead of having two cases\n",
    "    # where one case is t(s) and the other is t(s | s_prev)\n",
    "    predicted_tags = ['<start>']\n",
    "    for i in range(len(words)):\n",
    "        # skip the first/last word which is an injected token <start>/<end>\n",
    "        if i == 0 or i == len(words)-1:\n",
    "            continue\n",
    "        \n",
    "        word = words[i]\n",
    "        \n",
    "        # Keep counter of which tag has the best likelihood\n",
    "        best_tag = None\n",
    "        best_tag_value = 0\n",
    "        \n",
    "        # Enumerate through all possible transitions, and only consider those that fit the formula\n",
    "        # and those that do, we calculate its likelihood, and if its better than our best, we set it to our current best\n",
    "        for key, value in transition.items():\n",
    "            # key: (s, s')\n",
    "            # value: t(s'|s)\n",
    "            prev_tag, tag = key\n",
    "            \n",
    "            # skip the transitions that are not starting from our last predicted tag\n",
    "            if prev_tag != predicted_tags[-1]:\n",
    "                continue\n",
    "                \n",
    "            t = value # t(s' | s*) where s* is last predicted tag\n",
    "            e = 0\n",
    "            if (tag, word) in emission:\n",
    "                e = emission[(tag, word)]\n",
    "            computed = t * e\n",
    "            \n",
    "            if best_tag is None or computed > best_tag_value:\n",
    "                best_tag = tag\n",
    "                best_tag_value = computed\n",
    "        \n",
    "        # After looping through all transitions, we have found the best tag to predict\n",
    "        predicted_tags.append(best_tag)\n",
    "    predicted_tags.append('<end>')\n",
    "    \n",
    "    # Remove start/end tokens from the words and tags\n",
    "    words = words[1:-1]\n",
    "    predicted_tags = predicted_tags[1:-1]\n",
    "        \n",
    "    output.append({'words': words, 'tags': predicted_tags})\n",
    "    \n",
    "# Export result\n",
    "with open('greedy.out', 'w') as f:\n",
    "    for index, sentence in enumerate(output):\n",
    "        sentence_size = len(sentence['words'])\n",
    "        for i in range(sentence_size):\n",
    "            word = sentence['words'][i]\n",
    "            tag = sentence['tags'][i]\n",
    "            f.write(f'{i+1}\\t{word}\\t{tag}\\n')\n",
    "        # to match formatting, only add new line after each sentence if its not the last sentence\n",
    "        if index != len(output) - 1:\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding with HMM\n",
    "Here we implement and evalute the viterbi decoding algorithm on the development data and report the accuracy. Then, we preidct the pos tags on the test data and export the result into a file with format similar to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". = 0.06934691938123648\n",
      "freq = 37882\n",
      "\t('.', 'NNP') = 0.00139908135790085\n",
      "\t('.', ',') = 7.919328440948208e-05\n",
      "\t('.', 'CD') = 0.0001319888073491368\n",
      "\t('.', 'NNS') = 5.279552293965472e-05\n",
      "\t('.', 'JJ') = 0.00010559104587930944\n",
      "\t('.', 'MD') = 2.639776146982736e-05\n",
      "\t('.', 'VB') = 0.0003167731376379283\n",
      "\t('.', 'DT') = 0.00010559104587930944\n",
      "\t('.', 'NN') = 0.0003167731376379283\n",
      "\t('.', 'IN') = 7.919328440948208e-05\n",
      "\t('.', '.') = 0.0001847843302887915\n",
      "\t('.', 'VBZ') = 2.639776146982736e-05\n",
      "\t('.', 'CC') = 2.639776146982736e-05\n",
      "\t('.', 'VBD') = 2.639776146982736e-05\n",
      "\t('.', 'VBN') = 2.639776146982736e-05\n",
      "\t('.', 'RB') = 7.919328440948208e-05\n",
      "\t('.', 'TO') = 2.639776146982736e-05\n",
      "\t('.', 'PRP') = 2.639776146982736e-05\n",
      "\t('.', 'VBP') = 2.639776146982736e-05\n",
      "\t('.', '``') = 0.0005279552293965472\n",
      "\t('.', \"''\") = 0.058523837178607256\n",
      "\t('.', 'WP') = 2.639776146982736e-05\n",
      "\t('.', ':') = 0.00023757985322844623\n",
      "\t('.', 'WRB') = 2.639776146982736e-05\n",
      "\t('.', 'NNPS') = 0.0001319888073491368\n",
      "\t('.', '-LRB-') = 0.001478274642310332\n",
      "\t('.', '-RRB-') = 0.005253154532495644\n",
      "\t('.', 'FW') = 2.639776146982736e-05\n",
      "\t('.', 'UH') = 5.279552293965472e-05\n",
      "<end> = 0\n",
      "freq = 38217\n",
      "'' = 0.6744186046511627\n",
      "freq = 6622\n",
      "\t(\"''\", 'NNP') = 0.06236786469344609\n",
      "\t(\"''\", ',') = 0.000906070673512534\n",
      "\t(\"''\", 'CD') = 0.0022651766837813348\n",
      "\t(\"''\", 'NNS') = 0.017517366354575657\n",
      "\t(\"''\", 'JJ') = 0.00694654183026276\n",
      "\t(\"''\", 'MD') = 0.0045303533675626695\n",
      "\t(\"''\", 'VB') = 0.0016611295681063123\n",
      "\t(\"''\", 'DT') = 0.037148897614013895\n",
      "\t(\"''\", 'NN') = 0.03578979160374509\n",
      "\t(\"''\", 'IN') = 0.0881908788885533\n",
      "\t(\"''\", '.') = 0.0024161884627000906\n",
      "\t(\"''\", 'VBZ') = 0.12896405919661733\n",
      "\t(\"''\", 'VBG') = 0.009211718514044096\n",
      "\t(\"''\", 'CC') = 0.035940803382663845\n",
      "\t(\"''\", 'VBD') = 0.08471760797342193\n",
      "\t(\"''\", 'VBN') = 0.0049833887043189366\n",
      "\t(\"''\", 'RB') = 0.008305647840531562\n",
      "\t(\"''\", 'TO') = 0.0160072485653881\n",
      "\t(\"''\", 'PRP') = 0.07928118393234672\n",
      "\t(\"''\", 'WDT') = 0.009060706735125339\n",
      "\t(\"''\", 'VBP') = 0.002718212020537602\n",
      "\t(\"''\", 'PRP$') = 0.001359106010268801\n",
      "\t(\"''\", 'JJS') = 0.00015101177891875566\n",
      "\t(\"''\", '``') = 0.0024161884627000906\n",
      "\t(\"''\", \"''\") = 0.006342494714587738\n",
      "\t(\"''\", 'WP') = 0.0019631531259438235\n",
      "\t(\"''\", ':') = 0.007852612503775294\n",
      "\t(\"''\", 'JJR') = 0.00015101177891875566\n",
      "\t(\"''\", 'WRB') = 0.0033222591362126247\n",
      "\t(\"''\", '$') = 0.00015101177891875566\n",
      "\t(\"''\", 'NNPS') = 0.00015101177891875566\n",
      "\t(\"''\", 'WP$') = 0.000453035336756267\n",
      "\t(\"''\", '-LRB-') = 0.007097553609181516\n",
      "\t(\"''\", '-RRB-') = 0.0040773180308064025\n",
      ": = 0.936111111111111\n",
      "freq = 4680\n",
      "\t(':', 'NNP') = 0.10170940170940171\n",
      "\t(':', ',') = 0.00042735042735042735\n",
      "\t(':', 'CD') = 0.11944444444444445\n",
      "\t(':', 'NNS') = 0.030555555555555555\n",
      "\t(':', 'JJ') = 0.03888888888888889\n",
      "\t(':', 'MD') = 0.010256410256410256\n",
      "\t(':', 'VB') = 0.009401709401709401\n",
      "\t(':', 'DT') = 0.11517094017094016\n",
      "\t(':', 'NN') = 0.0391025641025641\n",
      "\t(':', 'IN') = 0.07136752136752136\n",
      "\t(':', '.') = 0.01730769230769231\n",
      "\t(':', 'VBZ') = 0.017735042735042734\n",
      "\t(':', 'VBG') = 0.02200854700854701\n",
      "\t(':', 'CC') = 0.06431623931623931\n",
      "\t(':', 'VBD') = 0.017094017094017096\n",
      "\t(':', 'VBN') = 0.012393162393162393\n",
      "\t(':', 'RB') = 0.05405982905982906\n",
      "\t(':', 'TO') = 0.010042735042735043\n",
      "\t(':', 'PRP') = 0.03803418803418803\n",
      "\t(':', 'RBR') = 0.0008547008547008547\n",
      "\t(':', 'WDT') = 0.01217948717948718\n",
      "\t(':', 'VBP') = 0.011752136752136752\n",
      "\t(':', 'PRP$') = 0.006837606837606838\n",
      "\t(':', 'JJS') = 0.0010683760683760685\n",
      "\t(':', '``') = 0.06474358974358975\n",
      "\t(':', 'EX') = 0.0017094017094017094\n",
      "\t(':', \"''\") = 0.001282051282051282\n",
      "\t(':', 'WP') = 0.0057692307692307696\n",
      "\t(':', ':') = 0.0017094017094017094\n",
      "\t(':', 'JJR') = 0.003205128205128205\n",
      "\t(':', 'WRB') = 0.006837606837606838\n",
      "\t(':', '$') = 0.024145299145299146\n",
      "\t(':', 'NNPS') = 0.00042735042735042735\n",
      "\t(':', 'WP$') = 0.00021367521367521368\n",
      "\t(':', '-LRB-') = 0.00042735042735042735\n",
      "\t(':', 'PDT') = 0.00021367521367521368\n",
      "\t(':', 'RBS') = 0.00042735042735042735\n",
      "\t(':', 'FW') = 0.00021367521367521368\n",
      "\t(':', 'UH') = 0.00021367521367521368\n",
      "\t(':', 'SYM') = 0.00021367521367521368\n",
      "\t(':', 'LS') = 0.0019230769230769232\n",
      "\t(':', '#') = 0.00042735042735042735\n",
      "-RRB- = 0.8970476911430735\n",
      "freq = 1321\n",
      "\t('-RRB-', 'NNP') = 0.0348221044663134\n",
      "\t('-RRB-', ',') = 0.1680545041635125\n",
      "\t('-RRB-', 'CD') = 0.008327024981074944\n",
      "\t('-RRB-', 'NNS') = 0.021196063588190765\n",
      "\t('-RRB-', 'JJ') = 0.012869038607115822\n",
      "\t('-RRB-', 'MD') = 0.01968205904617714\n",
      "\t('-RRB-', 'VB') = 0.006813020439061317\n",
      "\t('-RRB-', 'DT') = 0.024224072672218017\n",
      "\t('-RRB-', 'NN') = 0.04996214988644966\n",
      "\t('-RRB-', 'IN') = 0.10900832702498107\n",
      "\t('-RRB-', '.') = 0.1400454201362604\n",
      "\t('-RRB-', 'VBZ') = 0.04844814534443603\n",
      "\t('-RRB-', 'VBG') = 0.003028009084027252\n",
      "\t('-RRB-', 'CC') = 0.05299015897047691\n",
      "\t('-RRB-', 'VBD') = 0.029523088569265707\n",
      "\t('-RRB-', 'VBN') = 0.006813020439061317\n",
      "\t('-RRB-', 'RB') = 0.016654049962149888\n",
      "\t('-RRB-', 'TO') = 0.02195306585919758\n",
      "\t('-RRB-', 'PRP') = 0.006813020439061317\n",
      "\t('-RRB-', 'WDT') = 0.004542013626040878\n",
      "\t('-RRB-', 'VBP') = 0.02043906131718395\n",
      "\t('-RRB-', 'PRP$') = 0.000757002271006813\n",
      "\t('-RRB-', '``') = 0.001514004542013626\n",
      "\t('-RRB-', 'WP') = 0.003028009084027252\n",
      "\t('-RRB-', ':') = 0.08251324753974262\n",
      "\t('-RRB-', 'JJR') = 0.000757002271006813\n",
      "\t('-RRB-', '$') = 0.000757002271006813\n",
      "\t('-RRB-', '-LRB-') = 0.000757002271006813\n",
      "\t('-RRB-', 'FW') = 0.000757002271006813\n",
      "SYM = 0.9090909090909092\n",
      "freq = 55\n",
      "\t('SYM', 'NNP') = 0.03636363636363636\n",
      "\t('SYM', 'CD') = 0.07272727272727272\n",
      "\t('SYM', 'NN') = 0.05454545454545454\n",
      "\t('SYM', 'IN') = 0.03636363636363636\n",
      "\t('SYM', 'VBZ') = 0.05454545454545454\n",
      "\t('SYM', 'RB') = 0.01818181818181818\n",
      "\t('SYM', ':') = 0.6\n",
      "\t('SYM', 'FW') = 0.03636363636363636\n"
     ]
    }
   ],
   "source": [
    "for tag1 in tag_frequencies.keys():\n",
    "    prob_sum = 0\n",
    "    for tag2 in tag_frequencies.keys():\n",
    "        if (tag1, tag2) in transition:\n",
    "            prob_sum += transition[(tag1, tag2)]\n",
    "    if prob_sum < 0.99:\n",
    "        print(tag1 + \" = \" + str(prob_sum))\n",
    "        print('freq = ' + str(tag_frequencies[tag1]))\n",
    "        for tag2 in tag_frequencies.keys():\n",
    "            if (tag1, tag2) in transition:\n",
    "                print(\"\\t\" + str((tag1, tag2)) + \" = \" + str(transition[(tag1, tag2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Accuracy:  0.9089\n"
     ]
    }
   ],
   "source": [
    "# Parse in the development data and filter using our earlier vocab\n",
    "sentences = read_sentences('data/dev', vocab)\n",
    "\n",
    "# Keep counters to calculate accuracy\n",
    "total_correct = 0\n",
    "total = 0\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence['words']\n",
    "    tags = sentence['tags'] # used only for accuracy measurement\n",
    "    \n",
    "    viterbi = {}\n",
    "    backpointer = {}\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        # skip the first/last word which is an injected token <start>/<end>\n",
    "        if i == 0 or i == len(words)-1:\n",
    "            continue\n",
    "        \n",
    "        word = words[i]\n",
    "        \n",
    "        # Initialization of viterbi on first word\n",
    "        if i == 1:\n",
    "            for tag in tag_frequencies.keys():\n",
    "                t_key = ('<start>', tag)\n",
    "                t = 0\n",
    "                if t_key in transition:\n",
    "                    t = transition[t_key]\n",
    "                e_key = (tag, word)\n",
    "                e = 0\n",
    "                if e_key in emission:\n",
    "                    e = emission[e_key]\n",
    "                viterbi[(1, tag)] = t * e\n",
    "                \n",
    "                # also init backpointer\n",
    "                backpointer[(1, tag)] = tag\n",
    "        else:\n",
    "            for tag_cur in tag_frequencies.keys():\n",
    "                viterbi[(i, tag_cur)] = 0\n",
    "                for tag_prev in tag_frequencies.keys():\n",
    "                    table_value = viterbi[(i-1, tag_prev)]\n",
    "                    t_key = (tag_prev, tag_cur)\n",
    "                    t = 0\n",
    "                    if t_key in transition:\n",
    "                        t = transition[t_key]\n",
    "                    e_key = (tag_cur, word)\n",
    "                    e = 0\n",
    "                    if e_key in emission:\n",
    "                        e = emission[e_key]\n",
    "                    viterbi_value = table_value * t * e\n",
    "                    if viterbi_value >= viterbi[(i, tag_cur)]:\n",
    "                        viterbi[(i, tag_cur)] = viterbi_value\n",
    "                        backpointer[(i, tag_cur)] = tag_prev\n",
    "        \n",
    "    t = len(words)-2 # -1 to start from last character, another -1 to skip <end>\n",
    "    p = None\n",
    "    predicted_tags = []\n",
    "    while t > 0: # Not >= 0 so we skip the <start>\n",
    "        p_val = 0\n",
    "        for x in tag_frequencies.keys():\n",
    "            x_val = viterbi[(t, x)]\n",
    "            if p is None or x_val >= p_val:\n",
    "                p = x\n",
    "                p_val = x_val\n",
    "        predicted_tags.append(p)\n",
    "        t -= 1\n",
    "   \n",
    "    # predicted_tags dont include <start> and <end> tokens\n",
    "    predicted_tags.reverse()\n",
    "    \n",
    "    # Remove start/end tokens from the tags for a fair unpadded accuracy measurement\n",
    "    tags = tags[1:-1]\n",
    "    \n",
    "    # Compare actual tags with our predicted tags, and calculate the accuracy\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i] == predicted_tags[i]:\n",
    "            total_correct += 1\n",
    "    total += len(tags)\n",
    "    \n",
    "print(f'Viterbi Accuracy: {total_correct / total : .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse in the test data and filter using our earlier vocab\n",
    "sentences = read_sentences_test('data/test', vocab)\n",
    "\n",
    "# A list to contain all of our sentence pos predictions\n",
    "output = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence['words']\n",
    "    \n",
    "    viterbi = {}\n",
    "    backpointer = {}\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        # skip the first/last word which is an injected token <start>/<end>\n",
    "        if i == 0 or i == len(words)-1:\n",
    "            continue\n",
    "        \n",
    "        word = words[i]\n",
    "        \n",
    "        # Initialization of viterbi on first word\n",
    "        if i == 1:\n",
    "            for tag in tag_frequencies.keys():\n",
    "                t_key = ('<start>', tag)\n",
    "                t = 0\n",
    "                if t_key in transition:\n",
    "                    t = transition[t_key]\n",
    "                e_key = (tag, word)\n",
    "                e = 0\n",
    "                if e_key in emission:\n",
    "                    e = emission[e_key]\n",
    "                viterbi[(1, tag)] = t * e\n",
    "                \n",
    "                # also init backpointer\n",
    "                backpointer[(1, tag)] = tag\n",
    "        else:\n",
    "            for tag_cur in tag_frequencies.keys():\n",
    "                viterbi[(i, tag_cur)] = 0\n",
    "                for tag_prev in tag_frequencies.keys():\n",
    "                    table_value = viterbi[(i-1, tag_prev)]\n",
    "                    t_key = (tag_prev, tag_cur)\n",
    "                    t = 0\n",
    "                    if t_key in transition:\n",
    "                        t = transition[t_key]\n",
    "                    e_key = (tag_cur, word)\n",
    "                    e = 0\n",
    "                    if e_key in emission:\n",
    "                        e = emission[e_key]\n",
    "                    viterbi_value = table_value * t * e\n",
    "                    if viterbi_value >= viterbi[(i, tag_cur)]:\n",
    "                        viterbi[(i, tag_cur)] = viterbi_value\n",
    "                        backpointer[(i, tag_cur)] = tag_prev\n",
    "        \n",
    "    t = len(words)-2 # -1 to start from last character, another -1 to skip <end>\n",
    "    p = None\n",
    "    predicted_tags = []\n",
    "    while t > 0: # Not >= 0 so we skip the <start>\n",
    "        p_val = 0\n",
    "        for x in tag_frequencies.keys():\n",
    "            x_val = viterbi[(t, x)]\n",
    "            if p is None or x_val >= p_val:\n",
    "                p = x\n",
    "                p_val = x_val\n",
    "        predicted_tags.append(p)\n",
    "        t -= 1\n",
    "   \n",
    "    # predicted_tags dont include <start> and <end> tokens\n",
    "    predicted_tags.reverse()\n",
    "    \n",
    "    # Remove start/end tokens from the words\n",
    "    words = words[1:-1]\n",
    "        \n",
    "    output.append({'words': words, 'tags': predicted_tags})\n",
    "    \n",
    "# Export result\n",
    "with open('viterbi.out', 'w') as f:\n",
    "    for index, sentence in enumerate(output):\n",
    "        sentence_size = len(sentence['words'])\n",
    "        for i in range(sentence_size):\n",
    "            word = sentence['words'][i]\n",
    "            tag = sentence['tags'][i]\n",
    "            f.write(f'{i+1}\\t{word}\\t{tag}\\n')\n",
    "        # to match formatting, only add new line after each sentence if its not the last sentence\n",
    "        if index != len(output) - 1:\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
